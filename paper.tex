\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024

% submissions should not be anonymous, so use the 
% [preprint] option:
\usepackage[preprint]{paper}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib,preprint]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{geometry}
\usepackage{longtable}
\usepackage{graphicx}


\title{Smart Model Elimination for Efficient Automated Machine Learning}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Eric Su Zhang\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  St. Mark's School of Texas\\
  10600 Preston Rd, Dallas, TX 75230\\
  \texttt{ericspring08@gmail.com} \\
  % examples of more authors
  \And
  Benjamin Standefer \\
  St. Mark's School of Texas \\
  10600 Preston Rd, Dallas, TX 75230 \\
  \texttt{bjmstandefer@gmail.com} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}
  Automated Machine Learning or AutoML has emerged as a popular field of research. Many of these frameworks optimize a form of model selection, however, they all require most models to be run and evaluated. We propose a novel framework that automatically eliminates models that are unlikely to be performant by training a Boosting Model on hundreds of kaggle datasets. Out of the 30 model options, our framework can predict a top five model within its top eight 80\% of the time.
\end{abstract}


\section{Introduction}

Machine learning (ML) is a type of artificial intelligence (AI) that uses a variety of algorithms to interpret data and extrapolate from it, making predictions based on observed trends. Individual models are trained on datasets to become smarter so as to make predictions based on new data. The development and optimization of these models is a time-consuming process involving statistical nuances and complex learning strategies. This has led to the emergence of Automated Machine Learning (AutoML) frameworks and research. 

One sector of the global industry that has the most potential for beneficial integration of AutoML frameworks is healthcare, particularly in developing countries. These countries have severe shortages of healthcare workers and limited tools for diagnosis. For example, Africa has 2.3 healthcare workers per 1000 individuals, while the Americas have 24.8 healthcare workers per 1000. The World Health Organization (WHO) emphasized that this deficit is growing every year and will likely reach 18 million personnel by 2030. The use of AI in healthcare has been varied. Medical AI's typically automate repetitive tasks, making time consumption a primary concern. This coupled with a lack of adequate resources makes designing faster diagnostic systems for developing countries' medical sectors a pivotal issue. 
%
Most AutoML frameworks implement some form of model selection where a pool of models are filtered until a final model is selected. However, these frameworks require that every model to be run to filter out. In theory, this means that much energy is spent training models that are unlikely to be selected. We propose SMEML, a novel model selection algorithm that automatically eliminates models that it believes will not be performant. 

\subsection{Survey}

\begin{longtable}{|l|l|l|l|l|l|l|}
\hline
\textbf{Factor} & \textbf{H2O AutoML} & \textbf{TPOT} & \textbf{MLJAR} & \textbf{FLAML} & \textbf{LightAutoML} & \textbf{GAMA} \\
\hline
\endfirsthead

\multicolumn{7}{c}%
{\tablename\ \thetable\ -- \textit{Continued from previous page}} \\
\hline
\textbf{Factor} & \textbf{H2O AutoML} & \textbf{TPOT} & \textbf{MLJAR} & \textbf{FLAML} & \textbf{LightAutoML} & \textbf{GAMA} \\
\hline
\endhead

\hline \multicolumn{7}{|r|}{\textit{Continued on next page}} \\
\hline
\endfoot

\hline
\endlastfoot

\textbf{Ease of Use} & Easy-to-use GUI and API & Python library, requires coding experience & Very user-friendly web interface, detailed API & Simple API & User-friendly API with minimal coding & Python library with fairly intuitive API\\
\hline
\textbf{Supported Algorithms} & Linear, Tree, DNN, Clustering, etc. & Genetic Programming & Linear, Tree, LightGBM, CatBoost, Clustering, etc. (no DNN) & Linear, Tree, Clustering - More limited and lightweight & Linear, LightGBM, Catboost, etc. & Linear, tree, SVM, etc. \\
\hline
\textbf{Feature Engineering} & Basic & Basic & Advanced - includes feature interactions & Limited & Advanced (automatic) & Basic \\
\hline
\textbf{Time Management} & Time Constraints are Settable & Generations and population size & Time constraints are Settable & Optimized for quick results & Settable time constraints & Settable time constraints\\
\hline
\textbf{Ensemble Methods} & Yes (stacking + blending) & Yes (stacking) & Yes (stacking + blending) & No & Yes (stacking) & Yes (stacking) \\
\hline
\textbf{Hyperparameter Tuning} & Automated & Genetic algorithm & Automated and customizable & Automated and efficient tuning & Automated & Automated \\
\hline
\textbf{Model Interpretability} & SHAP, LIME, PDPs, MOJO & POJO, etc. & Basic & SHAP, LIME, PDPs, etc. & Basic (permutation importance) & SHAP, PDPs, etc. & Basic (feature importance) \\
\hline
\textbf{Scalability} & Good (distributed computing) & Limited & Good & Good (resource-efficient) & Good (distributing computing) & Moderate \\
\hline
\textbf{Customization} & High & Moderate & High & Low-Moderate & Moderate & Moderate \\
\hline

\end{longtable}
Table 1: Comparison of six AutoML frameworks based on commonmly evaluated criteria as well as more obscure features relevant to the medical diagnosis process 

We systematically reviewed and compared six different AutoML frameworks, comparing every feature relevant to medical implementation. Of these six, we wanted to take a closer look at four: FLAML, H2O, MLJAR, and TPOT. By testing our new framework against these four in evaluating medical data, we hoped to delineate key weaknesses and strengths in our own design. 

Taking a closer look at four AutoML standards, we can observe pros and cons in their theoretical application to medicine and research. FLAML uses blend search hyper-parameter optimization and focuses on lightweight models, making it quick and applicable to repetitive tasks in the medical sphere. It supports imputation techniques for missing values with user specification and has an intuitive API, making it moderately user-friendly for minimally trained healthcare workers. H2O has a wider variety of supported algorithms that may or may not be appropriate for medical diagnosis, making it slower than FLAML on average. In one trial, more than half of FLAML's performances in one minute were better than or equal to H20's performances in one hour. H2O has automatic and multi-faceted methods of dealing with missing values and flexible interfaces in R and Python, making it intuitive for users. MLJAR uses more advanced algorithms such as light gradient boosting and neural networks. It automatically deals with missing values and is known for its automated user interface. TPOT uses genetic programming. It builds pipelines over multiple generations, which can be time consuming. It has built-in pre-processing for missing values, but the genetic programming can require fine-tuning from users. 

Keeping the factors we have laid out in mind, namely expeditious running, accuracy, and interpretability, we also uncovered a key continuity in framework design: frameworks are forced to run and train the majority of their available models to maximize accuracy at the expense of efficiency. Given the restricted nature of tabular data, there are some models that, in general, are less likely to perform well, thus running and training these models is frequently wasteful. Cutting these models completely, however, is not optimal either, as they are the best option in a minority of cases and give frameworks versatility. We hypothesized that using a combination of layered machine learning implementation in which we train a model to predict a basic framework's outputs and meta-feature extraction in which we accelerate the training process by reducing complicated data matrices to their measurable attributes would yield an equally versatile system that minimizes time spent on prediction while still maximizing accuracy and straightforward implementation in a medical context. 

\section{Methodology}

\subsection{Training}

Insert Table Here
Table 2: Meta-features extracted in simplifcation of the Kaggle dataset-optimal model dataset

We used a Kaggle API library to search Kaggle using 20 medically-relevant key words. We limited our search to tabular datasets and obtained 300 viable datasets for training. We ran the Kaggle datasets through a rudimentary framework, pairing them with ML algorithms predicted to perform most optimally on that data. These ML algorithms were untrained and were selected from a pool of 30 candidates. The resulting 2x300 matrix then went through manuel meta-feature extraction (See Table 2). This new matrix was then used to train an XGBoost algorithm using a rank-based prediction system. 

\subsection{Experiments}

\section{Discussion}

\begin{ack}

\end{ack}

\section*{References}

\medskip
{
\small

% references here

\end{document}
